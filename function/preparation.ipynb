{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade geopy gspread ipykernel matplotlib numpy openpyxl openai pandas pip plotly_express polars PyCap pygsheets python-dotenv pyspark seaborn setuptools scikit-learn streamlit tabulate tabula-py xlsx2csv\n",
    "# %pip install pandas==1.5.3 [pandas_on_spark]\n",
    "# %pip install distutils\n",
    "# %pip install pyarrow, pandasai\n",
    "%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessaery packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as spark_func\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "\n",
    "# Some settings\n",
    "str_file = \"fix_Peka40STR2023.txt\"\n",
    "address_cols = [\"NoKPKIR\", \"Alamat\", \"Poskod\", \"Bandar\", \"Negeri\", \"state\",]\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"ReplacePostalCode\").getOrCreate()\n",
    "\n",
    "# Read the csv file\n",
    "df = spark.read.options(delimiter=\"|\", header=True).csv(str_file)\n",
    "\n",
    "# Select the important columns for address, then cast string to postcode, capitalize address, city, state, create a column of state\n",
    "address_df = df.select(*address_cols[0:5])\\\n",
    "               .withColumn(\"Poskod\", df[\"Poskod\"].cast(\"string\"))\\\n",
    "               .withColumn(\"state\", spark_func.upper(\"Negeri\"))\n",
    "# address_df = address_df.withColumn(\"Alamat\", spark_func.regexp_replace(address_df[\"Alamat\"], \"^#\", \"\"))\n",
    "\n",
    "# To capitalize all the address related information\n",
    "for column in address_cols[1:]:\n",
    "    address_df = address_df.withColumn(column, spark_func.upper(spark_func.trim(column)))\n",
    "               \n",
    "# Create list of state\n",
    "# ['W.PERSEKUTUAN (KL)', 'JOHOR', 'KEDAH', 'PERAK', 'PERLIS', 'PULAU PINANG', 'W.PERSEKUTUAN (LABUAN)', \n",
    "#  'SELANGOR', 'TERENGGANU', 'NEGERI SEMBILAN', 'KELANTAN', 'SARAWAK', 'W.PERSEKUTUAN (PUTRAJAYA)', 'PAHANG', 'MELAKA', 'SABAH']\n",
    "state_df = address_df.groupBy(\"Negeri\").count()\n",
    "state_list = [item[\"Negeri\"] for item in state_df.sort(\"Negeri\").toLocalIterator()]\n",
    "\n",
    "# Create another state column by changing federal states' name\n",
    "state_change_dict = {\"W.PERSEKUTUAN (KL)\":\"KUALA LUMPUR\",\n",
    "                     \"W.PERSEKUTUAN (LABUAN)\":\"LABUAN\",\n",
    "                     \"W.PERSEKUTUAN (PUTRAJAYA)\":\"PUTRAJAYA\"}\n",
    "\n",
    "# Change the state name\n",
    "for key in state_change_dict:\n",
    "    address_df = address_df.withColumn(\"state\", spark_func.when(spark_func.col(\"state\") == key, state_change_dict[key])\\\n",
    "                                       .otherwise(spark_func.col(\"state\")))\n",
    "\n",
    "for item in reversed(address_cols[2:]):\n",
    "    # To replace all poskod, city and state in the address\n",
    "    address_df = address_df.withColumn(\"Alamat\", spark_func.regexp_replace(address_df[\"Alamat\"], address_df[item], \"\"))\n",
    "    # To remove all those end with ,\n",
    "    address_df = address_df.withColumn(\"Alamat\", spark_func.regexp_replace(address_df[\"Alamat\"], \",\\\\s*$\", \"\"))\n",
    "    address_df = address_df.withColumn(\"Alamat\", spark_func.regexp_replace(address_df[\"Alamat\"], \",\\\\s*\", \", \"))\n",
    "    address_df = address_df.withColumn(\"Alamat\", spark_func.regexp_replace(address_df[\"Alamat\"], \",\\\\s*,\\\\s*\", \", \"))\n",
    "    address_df = address_df.withColumn(\"Alamat\", spark_func.regexp_replace(address_df[\"Alamat\"], \"\\\\s*,\", \",\"))\n",
    "    address_df = address_df.withColumn(\"Alamat\", spark_func.trim(spark_func.regexp_replace(address_df[\"Alamat\"], \"\\\\s+\", \" \")))\n",
    "    \n",
    "address_df = address_df.withColumn(\"address\", spark_func.when(spark_func.col(\"Alamat\") != \"\", spark_func.concat_ws(\" \", *address_cols[1:4], \"state\")))\n",
    "address_df1 = address_df.filter(spark_func.col(\"address\").isNotNull())\n",
    "# address_df1.dropDuplicates([\"address\"])\n",
    "address_df1.select(address_cols[0], \"address\").show(100, truncate = False)\n",
    "\n",
    "# for value in reversed(address_cols[2:]):\n",
    "    # print(item)\n",
    "    # expr(f\"regexp_replace(Alamat, '{col_name}$', '')\"))\n",
    "    # spark_func.expr(spark_func.trim(address_df[\"Alamat\"]), address_df[item], \"\"))\n",
    "    # address_df = address_df.withColumn(\"Alamat\", spark_func.regexp_replace(spark_func.trim(address_df[\"Alamat\"]), f'\\\\b{item}\\\\b$', ''))\n",
    "    # address_df = address_df.withColumn(\"Alamat\", spark_func.expr(f\"CASE WHEN split(Alamat, ' ')[-1] = '{value}' THEN regexp_replace(Alamat, ' {value}$', '') ELSE Alamat END\"))\n",
    "\n",
    "# df1 = address_df1.dropDuplicates([\"address\"]).select(address_cols[0], \"address\")\n",
    "# df1.show(truncate = False)\n",
    "address_df1.select(address_cols[0], \"address\")\\\n",
    "    .coalesce(1).write.format(\"csv\").mode('overwrite').options(header='True', delimiter='|')\\\n",
    "        .save(\"str_address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_address(df, columns_to_loop, column_need_to_be_clean,):\n",
    "    for value in columns_to_loop:\n",
    "        df = df.withColumn(\"words\", spark_func.split(column_need_to_be_clean, \"\\\\s+\"))\\\n",
    "               .withColumn(\"last\", spark_func.expr(\"words[size(words) - 1]\"))\\\n",
    "               .withColumn(\"words\", spark_func.expr(\"slice(words, 1, size(words) - 1)\"))\n",
    "        df = df.withColumn(\"last\", spark_func.when(df[value]==df[\"last\"], \"\").otherwise(df[\"last\"]))\n",
    "        df = df.withColumn(column_need_to_be_clean, spark_func.trim(spark_func.concat_ws(\" \", df[\"words\"], df[\"last\"])))\n",
    "        df = df.withColumn(column_need_to_be_clean, spark_func.regexp_replace(df[column_need_to_be_clean], \",\\\\s*$\", \"\"))\n",
    "        df = df.withColumn(column_need_to_be_clean, spark_func.regexp_replace(df[column_need_to_be_clean], \",\\\\s*\", \", \"))\n",
    "        df = df.withColumn(column_need_to_be_clean, spark_func.regexp_replace(df[column_need_to_be_clean], \",\\\\s*,\\\\s*\", \", \"))\n",
    "        df = df.withColumn(column_need_to_be_clean, spark_func.regexp_replace(df[column_need_to_be_clean], \"\\\\s*,\", \",\"))\n",
    "        df = df.withColumn(column_need_to_be_clean, spark_func.trim(spark_func.regexp_replace(df[column_need_to_be_clean], \"\\\\s+\", \" \")))\n",
    "        return df.drop(\"words\", \"last\")\n",
    "clean_address(address_df, reversed(address_cols[2:]), \"Alamat\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessaery packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as spark_func\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "\n",
    "# Some settings\n",
    "str_file = \"fix_Peka40STR2023.txt\"\n",
    "address_cols = [\"NoKPKIR\", \"Alamat\", \"Poskod\", \"Bandar\", \"Negeri\", \"state\",]\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"ReplacePostalCode\").getOrCreate()\n",
    "\n",
    "# Read the csv file\n",
    "df = spark.read.options(delimiter=\"|\", header=True).csv(\"str_address\")\n",
    "df = df.pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.ExcelWriter(\"str_address.xlsx\", mode=\"w\") as writer:\n",
    "#     for num in range(0, len(df), 1000):\n",
    "#         df.iloc[num:num+1000].to_excel(writer, sheet_name = f\"sheet{num/1000}\", index = False)\n",
    "\n",
    "for num in range(0, len(df), 1000):\n",
    "    df.iloc[num:num+1000].to_excel(f\"str_partition/sheet{num/1000}.xlsx\", sheet_name = f\"sheet{num//1000}\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "df = pd.read_csv(\"str_address.csv\", sep=\"|\", error = 'coerce')\n",
    "\n",
    "# Create a connection to a new SQLite database\n",
    "conn = sqlite3.connect('str.db')\n",
    "\n",
    "# Write the DataFrame to a table in the SQLite database\n",
    "df.to_sql('str', conn, if_exists='replace', index=True)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# df = pd.read_csv(\"str_address.csv\", sep=\"|\")\n",
    "\n",
    "# Create a connection to a new SQLite database\n",
    "conn = sqlite3.connect(r'str_address/str.db')\n",
    "str_db = sqlite3.connect(r\"str_address/str_simple.db\")\n",
    "\n",
    "# conn.cursor().execute(\"ALTER TABLE raw DROP COLUMN NoKPKIR\")\n",
    "# conn.commit()\n",
    "# pd.read_sql(\"ALTER TABLE str ADD COLUMN point CHAR\", conn)\n",
    "# pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table'\", conn).name\n",
    "for chunk in pd.read_sql(\"SELECT id, NoKPKIR, address FROM str\", con=conn, chunksize=10000):\n",
    "    chunk.to_sql(\"str\", con = str_db, index=False, if_exists=\"append\")\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def read_file(path:str, method:str) -> pd.DataFrame:\n",
    "    if method == \"csv\":\n",
    "        return pd.concat([pd.read_csv(f\"{path}/{file}\", sep=\"|\")\n",
    "                          for file in os.listdir(path)\n",
    "                          if file.endswith(\"csv\")], ignore_index=True).drop_duplicates()\n",
    "\n",
    "    elif method == \"excel\":\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for file in [file for file in os.listdir(path) if file.endswith(\"xlsx\")]:\n",
    "            if len(pd.ExcelFile(f\"{path}/{file}\").sheet_names) > 1:\n",
    "                temp_df = pd.concat([pd.read_excel(f\"{path}/{file}\", sheet_name=num) \n",
    "                                     for num in range(len(pd.ExcelFile(f\"{path}/{file}\").sheet_names))])\n",
    "            else:\n",
    "                temp_df = pd.read_excel(f\"{path}/{file}\")\n",
    "            df = pd.concat([df, temp_df], ignore_index=True)\n",
    "        \n",
    "        return df.drop_duplicates()\n",
    "    \n",
    "def convert_point_to_lat_lon(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    # Create a copy to prevent loss of original data\n",
    "    temp_df = df.copy(deep = True)\n",
    "    try:\n",
    "        # Temporary replace the (, to reduce memory usage?\n",
    "        temp_df.loc[:,\"temp\"] = temp_df.loc[:,\"point\"].astype(str).str.strip(\"()\")\n",
    "        # Split it and put the 0 to latitude and 1 to longitude\n",
    "        temp_df.loc[:,\"Latitude\"] = temp_df.loc[:,\"point\"].apply(lambda x: str(x).split(\",\")[0])\n",
    "        temp_df.loc[:,\"Longitude\"] = temp_df.loc[:,\"point\"].apply(lambda x: str(x).split(\",\")[1])\n",
    "        # Return the temp_df after drop columns for \"temp\"\n",
    "        return temp_df.drop(columns = \"temp\")\n",
    "    except:\n",
    "        # Loop through for some times if hte method above failed\n",
    "        for index, row in temp_df.iterrows():\n",
    "            # Split the content\n",
    "            temp_str = row[\"point\"].split(\",\")\n",
    "            try:\n",
    "                temp_df.loc[index, \"Latitude\"] = float(temp_str[0].replace(\"(\", \"\"))\n",
    "                temp_df.loc[index, \"Longitude\"] = float(temp_str[1])\n",
    "            except:\n",
    "                continue\n",
    "        # Return the temp_df\n",
    "        return temp_df\n",
    "\n",
    "def convert_id_to_kpir(df:pd.DataFrame, \n",
    "                       id_columns:str = \"id\",\n",
    "                       database_file:str = 'str_address/str.db',\n",
    "                       columns:tuple = (\"Latitude\", \"Longitude\",)) -> pd.DataFrame:\n",
    "    import sqlite3\n",
    "\n",
    "    # Prepare for SQL statement\n",
    "    id_list = tuple(df.loc[:,id_columns].astype(int).unique())\n",
    "\n",
    "    # Get the data from database\n",
    "    return pd.read_sql(f\"SELECT * FROM str WHERE id IN{id_list}\", con = sqlite3.connect(database_file))\\\n",
    "                .merge(df.loc[:,(id_columns,) + columns], \n",
    "                       how = \"outer\", left_on = id_columns, right_on = \"id\")\n",
    "\n",
    "# previous_csv = read_file(\"str_partition\", \"csv\") # 319760\n",
    "# previous_csv_no_point = previous_csv.query(\"point.isnull()\") # 36292\n",
    "# previous_csv_point = convert_point_to_lat_lon(previous_csv.query(\"point.notnull()\")) # 283468\n",
    "# str_partition_complete = read_file(\"str_partition/completed\", \"excel\") # Done\n",
    "# str_partition_with_id = str_partition_complete.query(\"id.notnull()\") # 25966 Done\n",
    "# str_partition_with_id.loc[:,\"ic\"] = str_partition_with_id.loc[:,\"id\"].apply(lambda x: len(str(x))) # Done\n",
    "# str_partition_with_ic = pd.concat([str_partition_complete.query(\"NoKPKIR.notnull()\"), \n",
    "#                                    str_partition_complete.query(\"ic.notnull()\")\\\n",
    "#                                                          .drop(columns = \"NoKPKIR\")\\\n",
    "#                                                          .rename(columns = {\"ic\":\"NoKPKIR\"}),\n",
    "#                                    str_partition_with_id.query(\"ic == 14\")\\\n",
    "#                                                         .drop(columns = [\"NoKPKIR\", \"ic\"])\\\n",
    "#                                                         .rename(columns = {\"id\":\"NoKPKIR\"}), \n",
    "#                                    pd.read_sql(f\"SELECT * FROM str WHERE id IN{tuple(str_partition_with_id.query(\"ic == 9\").loc[:,\"id\"].astype(int).unique())}\", \n",
    "#                                                con=conn)\\\n",
    "#                                      .merge(str_partition_with_id.loc[str_partition_with_id.loc[:,\"ic\"] == 9,\n",
    "#                                                                       (\"id\", \"Latitude\", \"Longitude\")], \n",
    "#                                             how = \"outer\", on = \"id\")],\n",
    "#                                   ignore_index=True).drop_duplicates(subset=[\"NoKPKIR\", \"address\"]) # 283463\n",
    "\n",
    "# read_file(\"str_google\", \"excel\").info() -> no point/lat/lon\n",
    "\n",
    "# completed_id =  convert_id_to_kpir(convert_point_to_lat_lon(read_file(\"completed/id\", \"excel\"))) # 10385\n",
    "# completed_kpir = read_file(\"completed\", \"excel\") # 66923\n",
    "# completed_kpir_lat_lon = completed_kpir.query(\"Latitude.notnull()\") #16457\n",
    "# completed_kpir_point = completed_kpir.query(\"Latitude.isnull()\") # 50466\n",
    "# completed_kpir_full = pd.concat([convert_point_to_lat_lon(completed_kpir_point).drop(columns=\"temp\"), \n",
    "#                                  completed_kpir_lat_lon], ignore_index=True)\n",
    "# completed_kpir_fuck = completed_kpir_full.query(\"Latitude.isnull()\")\n",
    "# for index, row in completed_kpir_fuck.iterrows():\n",
    "#     # Split the content\n",
    "#     temp_str = row[\"point\"].split(\",\")\n",
    "#     try:\n",
    "#         completed_kpir_fuck.loc[index, \"Latitude\"] = float(temp_str[0].replace(\"(\", \"\"))\n",
    "#         completed_kpir_fuck.loc[index, \"Longitude\"] = float(temp_str[1])\n",
    "#     except:\n",
    "#         continue\n",
    "# final_df = pd.concat([completed_kpir_full, completed_kpir_fuck], ignore_index=True)\\\n",
    "#              .sort_values(\"NoKPKIR\")\\\n",
    "#                 .drop_duplicates(subset=[\"NoKPKIR\", \"address\"])\n",
    "# final_df.to_csv(\"test_geo.csv\", index = False, sep=\"|\") # 66923\n",
    "\n",
    "# new_str = read_file(\"new_str\", \"excel\") -> no point/lat/lon\n",
    "# new_str_complete_point = read_file(\"new_str/complete\", \"excel\")\n",
    "# new_str_complete_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_str_complete_point = read_file(\"new_str/complete\", \"excel\")\n",
    "\n",
    "# for index, row in new_str_complete_point.iterrows():\n",
    "#     # Loop through for some times if hte method above failed\n",
    "#     temp_str = row[\"point\"].split(\",\")\n",
    "#     try:\n",
    "#         new_str_complete_point.loc[index, \"Latitude\"] = float(temp_str[0].replace(\"(\", \"\"))\n",
    "#         new_str_complete_point.loc[index, \"Longitude\"] = float(temp_str[1])\n",
    "#     except:\n",
    "#         continue\n",
    "\n",
    "# # new_str_complete_point.to_parquet(\"test_geo2.parquet\")\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "# pl.from_pandas(final_df2)\n",
    "# for column in final_df.columns:\n",
    "#     try:\n",
    "#         final_df.loc[:,column] = pd.to_numeric(final_df2.loc[:,column], downcast=\"integer\")\n",
    "#     except:\n",
    "#         continue\n",
    "# final_df = final_df.loc[:,(\"NoKPKIR\", \"address\", \"Latitude\", \"Longitude\")]\n",
    "# final_df.loc[:,\"NoKPKIR\"] = final_df.loc[:,\"NoKPKIR\"].astype(int)\n",
    "# kpir_issue = pl.from_pandas(new_str_complete_point.query(\"NoKPKIR.notnull()\"))\\\n",
    "#     .select(pl.col(\"NoKPKIR\").cast(pl.Int64), \"address\", \"Latitude\", \"Longitude\")\\\n",
    "#     .to_pandas()\n",
    "\n",
    "\n",
    "# final_df = pd.concat([convert_id_to_kpir(new_str_complete_point.query(\"NoKPKIR.isnull()\")),\n",
    "#                       kpir_issue], ignore_index=True).drop(columns = \"id\")\n",
    "\n",
    "# final_df2 = pd.concat([final_df.drop_duplicates(subset = [\"NoKPKIR\", \"address\"]),\n",
    "#                        .drop_duplicates(subset=[\"NoKPKIR\", \"address\"]),\n",
    "#                        pd.read_csv(\"test_geo.csv\", sep = \"|\").drop_duplicates(subset=[\"NoKPKIR\", \"address\"]),\n",
    "#                        pd.read_parquet(\"Final Geo Data.parquet\")], ignore_index=True)\n",
    "\n",
    "\n",
    "# test_geo = pd.read_parquet(\"test_geo.parquet\").loc[:,(\"NoKPKIR\", \"Latitude\", \"Longitude\")]\n",
    "# little_wife = pd.read_parquet(\"Final Geo Data.parquet\").loc[:,(\"NoKPKIR\", \"Latitude\", \"Longitude\")]\n",
    "# csv_df = pl.read_csv(\"test_geo.csv\", separator =\"|\", ignore_errors=True)\\\n",
    "#            .select(pl.col(\"NoKPKIR\").cast(pl.Int64), \"Latitude\", \"Longitude\").to_pandas()\n",
    "\n",
    "# final_df2 = pd.concat([final_df.loc[:,(\"NoKPKIR\", \"Latitude\", \"Longitude\")],\n",
    "#                        test_geo, little_wife, csv_df], ignore_index=True)\n",
    "final_df2.drop_duplicates(subset=\"NoKPKIR\").to_parquet(\"2024_07_02.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.read_parquet(\"2024_07_03.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "pl.read_csv(\"fix_Peka40STR2023.txt\", separator=\"|\", ignore_errors = True)\\\n",
    "    .join(pl.read_parquet(\"2024_07_03.parquet\").drop(\"__index_level_0__\"), on= \"NoKPKIR\", how = \"left\")\\\n",
    "    .write_parquet(\"2024_07_02.parquet\", use_pyarrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import polars as pl\n",
    "# df = pl.read_parquet(\"2024_07_02.parquet\")\n",
    "# df.filter(pl.col(\"Latitude\").is_not_null())\n",
    "# df.group_by(\"Negeri\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "address_df = pd.read_sql(\"SELECT NoKPKIR, address FROM str\", \n",
    "                         con = sqlite3.connect(r\"/Users/wh0102/Downloads/github/str/str_address/str.db\"))\n",
    "\n",
    "original_df = pd.read_parquet(\"2024_07_02.parquet\")\n",
    "address_df.merge(original_df, \n",
    "                 how = \"outer\", \n",
    "                 on = \"NoKPKIR\").to_sql(\"str\", \n",
    "                                        con = sqlite3.connect(\"str_half.db\"),\n",
    "                                        index_label=\"id\",\n",
    "                                        if_exists=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.ExcelWriter(\"str_address/siswa.xlsx\", engine = \"openpyxl\", mode = \"w\") as writer:\n",
    "#     for num in range(0, 500000, 10000):\n",
    "#         df.iloc[num:num+10000].to_excel(writer, sheet_name = f\"sheet{int(num/10000)}\", index = False)\n",
    "\n",
    "# with pd.ExcelWriter(\"str_address/google.xlsx\", engine = \"openpyxl\", mode = \"w\") as writer:\n",
    "#     for num in range(500000, len(df), 10000):\n",
    "#         df.iloc[num:num+10000].to_excel(writer, sheet_name = f\"sheet{int(num/10000)}\", index = False)\n",
    "\n",
    "for num in range(0, len(df), 10000):\n",
    "    df.iloc[num:num+10000].to_excel(f\"str_google/google/sheet{int(num/10000)}.xlsx\", index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
